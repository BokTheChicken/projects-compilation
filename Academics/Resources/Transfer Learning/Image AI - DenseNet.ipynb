{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from imageai.Prediction.Custom import ModelTraining\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from distutils.version import LooseVersion\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.14.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer.' \\\n",
    "                                                            '  You are using {}'.format(tf.__version__)\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "#Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please ensure you have installed tensorflow correctly')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer = ModelTraining()\n",
    "trainer.setModelTypeAsInceptionV3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 111, 111, 32) 864         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 111, 111, 32) 96          conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_95 (Activation)      (None, 111, 111, 32) 0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 109, 109, 32) 9216        activation_95[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, 109, 109, 32) 96          conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_96 (Activation)      (None, 109, 109, 32) 0           batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 109, 109, 64) 18432       activation_96[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, 109, 109, 64) 192         conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_97 (Activation)      (None, 109, 109, 64) 0           batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 54, 54, 64)   0           activation_97[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 54, 54, 80)   5120        max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, 54, 54, 80)   240         conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_98 (Activation)      (None, 54, 54, 80)   0           batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, 52, 52, 192)  138240      activation_98[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, 52, 52, 192)  576         conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_99 (Activation)      (None, 52, 52, 192)  0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 25, 25, 192)  0           activation_99[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, 25, 25, 64)   12288       max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (None, 25, 25, 64)   192         conv2d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_103 (Activation)     (None, 25, 25, 64)   0           batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, 25, 25, 48)   9216        max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 25, 25, 96)   55296       activation_103[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, 25, 25, 48)   144         conv2d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, 25, 25, 96)   288         conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_101 (Activation)     (None, 25, 25, 48)   0           batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_104 (Activation)     (None, 25, 25, 96)   0           batch_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_10 (AveragePo (None, 25, 25, 192)  0           max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, 25, 25, 64)   12288       max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, 25, 25, 64)   76800       activation_101[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, 25, 25, 96)   82944       activation_104[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)             (None, 25, 25, 32)   6144        average_pooling2d_10[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, 25, 25, 64)   192         conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, 25, 25, 64)   192         conv2d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_105 (BatchN (None, 25, 25, 96)   288         conv2d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_106 (BatchN (None, 25, 25, 32)   96          conv2d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_100 (Activation)     (None, 25, 25, 64)   0           batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_102 (Activation)     (None, 25, 25, 64)   0           batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_105 (Activation)     (None, 25, 25, 96)   0           batch_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_106 (Activation)     (None, 25, 25, 32)   0           batch_normalization_106[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, 25, 25, 256)  0           activation_100[0][0]             \n",
      "                                                                 activation_102[0][0]             \n",
      "                                                                 activation_105[0][0]             \n",
      "                                                                 activation_106[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_110 (Conv2D)             (None, 25, 25, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_110 (BatchN (None, 25, 25, 64)   192         conv2d_110[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_110 (Activation)     (None, 25, 25, 64)   0           batch_normalization_110[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_108 (Conv2D)             (None, 25, 25, 48)   12288       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_111 (Conv2D)             (None, 25, 25, 96)   55296       activation_110[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_108 (BatchN (None, 25, 25, 48)   144         conv2d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_111 (BatchN (None, 25, 25, 96)   288         conv2d_111[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_108 (Activation)     (None, 25, 25, 48)   0           batch_normalization_108[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_111 (Activation)     (None, 25, 25, 96)   0           batch_normalization_111[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_11 (AveragePo (None, 25, 25, 256)  0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_107 (Conv2D)             (None, 25, 25, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_109 (Conv2D)             (None, 25, 25, 64)   76800       activation_108[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_112 (Conv2D)             (None, 25, 25, 96)   82944       activation_111[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_113 (Conv2D)             (None, 25, 25, 64)   16384       average_pooling2d_11[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_107 (BatchN (None, 25, 25, 64)   192         conv2d_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_109 (BatchN (None, 25, 25, 64)   192         conv2d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_112 (BatchN (None, 25, 25, 96)   288         conv2d_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_113 (BatchN (None, 25, 25, 64)   192         conv2d_113[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_107 (Activation)     (None, 25, 25, 64)   0           batch_normalization_107[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_109 (Activation)     (None, 25, 25, 64)   0           batch_normalization_109[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_112 (Activation)     (None, 25, 25, 96)   0           batch_normalization_112[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_113 (Activation)     (None, 25, 25, 64)   0           batch_normalization_113[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, 25, 25, 288)  0           activation_107[0][0]             \n",
      "                                                                 activation_109[0][0]             \n",
      "                                                                 activation_112[0][0]             \n",
      "                                                                 activation_113[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_117 (Conv2D)             (None, 25, 25, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_117 (BatchN (None, 25, 25, 64)   192         conv2d_117[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_117 (Activation)     (None, 25, 25, 64)   0           batch_normalization_117[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_115 (Conv2D)             (None, 25, 25, 48)   13824       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_118 (Conv2D)             (None, 25, 25, 96)   55296       activation_117[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_115 (BatchN (None, 25, 25, 48)   144         conv2d_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_118 (BatchN (None, 25, 25, 96)   288         conv2d_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_115 (Activation)     (None, 25, 25, 48)   0           batch_normalization_115[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_118 (Activation)     (None, 25, 25, 96)   0           batch_normalization_118[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_12 (AveragePo (None, 25, 25, 288)  0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_114 (Conv2D)             (None, 25, 25, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_116 (Conv2D)             (None, 25, 25, 64)   76800       activation_115[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_119 (Conv2D)             (None, 25, 25, 96)   82944       activation_118[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_120 (Conv2D)             (None, 25, 25, 64)   18432       average_pooling2d_12[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_114 (BatchN (None, 25, 25, 64)   192         conv2d_114[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_116 (BatchN (None, 25, 25, 64)   192         conv2d_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_119 (BatchN (None, 25, 25, 96)   288         conv2d_119[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_120 (BatchN (None, 25, 25, 64)   192         conv2d_120[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_114 (Activation)     (None, 25, 25, 64)   0           batch_normalization_114[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_116 (Activation)     (None, 25, 25, 64)   0           batch_normalization_116[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_119 (Activation)     (None, 25, 25, 96)   0           batch_normalization_119[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_120 (Activation)     (None, 25, 25, 64)   0           batch_normalization_120[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, 25, 25, 288)  0           activation_114[0][0]             \n",
      "                                                                 activation_116[0][0]             \n",
      "                                                                 activation_119[0][0]             \n",
      "                                                                 activation_120[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_122 (Conv2D)             (None, 25, 25, 64)   18432       mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_122 (BatchN (None, 25, 25, 64)   192         conv2d_122[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_122 (Activation)     (None, 25, 25, 64)   0           batch_normalization_122[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_123 (Conv2D)             (None, 25, 25, 96)   55296       activation_122[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_123 (BatchN (None, 25, 25, 96)   288         conv2d_123[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_123 (Activation)     (None, 25, 25, 96)   0           batch_normalization_123[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_121 (Conv2D)             (None, 12, 12, 384)  995328      mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_124 (Conv2D)             (None, 12, 12, 96)   82944       activation_123[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_121 (BatchN (None, 12, 12, 384)  1152        conv2d_121[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_124 (BatchN (None, 12, 12, 96)   288         conv2d_124[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_121 (Activation)     (None, 12, 12, 384)  0           batch_normalization_121[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_124 (Activation)     (None, 12, 12, 96)   0           batch_normalization_124[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 12, 12, 288)  0           mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)            (None, 12, 12, 768)  0           activation_121[0][0]             \n",
      "                                                                 activation_124[0][0]             \n",
      "                                                                 max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_129 (Conv2D)             (None, 12, 12, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_129 (BatchN (None, 12, 12, 128)  384         conv2d_129[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_129 (Activation)     (None, 12, 12, 128)  0           batch_normalization_129[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_130 (Conv2D)             (None, 12, 12, 128)  114688      activation_129[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_130 (BatchN (None, 12, 12, 128)  384         conv2d_130[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_130 (Activation)     (None, 12, 12, 128)  0           batch_normalization_130[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_126 (Conv2D)             (None, 12, 12, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_131 (Conv2D)             (None, 12, 12, 128)  114688      activation_130[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_126 (BatchN (None, 12, 12, 128)  384         conv2d_126[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_131 (BatchN (None, 12, 12, 128)  384         conv2d_131[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_126 (Activation)     (None, 12, 12, 128)  0           batch_normalization_126[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_131 (Activation)     (None, 12, 12, 128)  0           batch_normalization_131[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_127 (Conv2D)             (None, 12, 12, 128)  114688      activation_126[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_132 (Conv2D)             (None, 12, 12, 128)  114688      activation_131[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_127 (BatchN (None, 12, 12, 128)  384         conv2d_127[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_132 (BatchN (None, 12, 12, 128)  384         conv2d_132[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_127 (Activation)     (None, 12, 12, 128)  0           batch_normalization_127[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_132 (Activation)     (None, 12, 12, 128)  0           batch_normalization_132[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_13 (AveragePo (None, 12, 12, 768)  0           mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_125 (Conv2D)             (None, 12, 12, 192)  147456      mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_128 (Conv2D)             (None, 12, 12, 192)  172032      activation_127[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_133 (Conv2D)             (None, 12, 12, 192)  172032      activation_132[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_134 (Conv2D)             (None, 12, 12, 192)  147456      average_pooling2d_13[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_125 (BatchN (None, 12, 12, 192)  576         conv2d_125[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_128 (BatchN (None, 12, 12, 192)  576         conv2d_128[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_133 (BatchN (None, 12, 12, 192)  576         conv2d_133[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_134 (BatchN (None, 12, 12, 192)  576         conv2d_134[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_125 (Activation)     (None, 12, 12, 192)  0           batch_normalization_125[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_128 (Activation)     (None, 12, 12, 192)  0           batch_normalization_128[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_133 (Activation)     (None, 12, 12, 192)  0           batch_normalization_133[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_134 (Activation)     (None, 12, 12, 192)  0           batch_normalization_134[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)            (None, 12, 12, 768)  0           activation_125[0][0]             \n",
      "                                                                 activation_128[0][0]             \n",
      "                                                                 activation_133[0][0]             \n",
      "                                                                 activation_134[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_139 (Conv2D)             (None, 12, 12, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_139 (BatchN (None, 12, 12, 160)  480         conv2d_139[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_139 (Activation)     (None, 12, 12, 160)  0           batch_normalization_139[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_140 (Conv2D)             (None, 12, 12, 160)  179200      activation_139[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_140 (BatchN (None, 12, 12, 160)  480         conv2d_140[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_140 (Activation)     (None, 12, 12, 160)  0           batch_normalization_140[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_136 (Conv2D)             (None, 12, 12, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_141 (Conv2D)             (None, 12, 12, 160)  179200      activation_140[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_136 (BatchN (None, 12, 12, 160)  480         conv2d_136[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_141 (BatchN (None, 12, 12, 160)  480         conv2d_141[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_136 (Activation)     (None, 12, 12, 160)  0           batch_normalization_136[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_141 (Activation)     (None, 12, 12, 160)  0           batch_normalization_141[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_137 (Conv2D)             (None, 12, 12, 160)  179200      activation_136[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_142 (Conv2D)             (None, 12, 12, 160)  179200      activation_141[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_137 (BatchN (None, 12, 12, 160)  480         conv2d_137[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_142 (BatchN (None, 12, 12, 160)  480         conv2d_142[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_137 (Activation)     (None, 12, 12, 160)  0           batch_normalization_137[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_142 (Activation)     (None, 12, 12, 160)  0           batch_normalization_142[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_14 (AveragePo (None, 12, 12, 768)  0           mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_135 (Conv2D)             (None, 12, 12, 192)  147456      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_138 (Conv2D)             (None, 12, 12, 192)  215040      activation_137[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_143 (Conv2D)             (None, 12, 12, 192)  215040      activation_142[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_144 (Conv2D)             (None, 12, 12, 192)  147456      average_pooling2d_14[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_135 (BatchN (None, 12, 12, 192)  576         conv2d_135[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_138 (BatchN (None, 12, 12, 192)  576         conv2d_138[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_143 (BatchN (None, 12, 12, 192)  576         conv2d_143[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_144 (BatchN (None, 12, 12, 192)  576         conv2d_144[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_135 (Activation)     (None, 12, 12, 192)  0           batch_normalization_135[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_138 (Activation)     (None, 12, 12, 192)  0           batch_normalization_138[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_143 (Activation)     (None, 12, 12, 192)  0           batch_normalization_143[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_144 (Activation)     (None, 12, 12, 192)  0           batch_normalization_144[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)            (None, 12, 12, 768)  0           activation_135[0][0]             \n",
      "                                                                 activation_138[0][0]             \n",
      "                                                                 activation_143[0][0]             \n",
      "                                                                 activation_144[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_149 (Conv2D)             (None, 12, 12, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_149 (BatchN (None, 12, 12, 160)  480         conv2d_149[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_149 (Activation)     (None, 12, 12, 160)  0           batch_normalization_149[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_150 (Conv2D)             (None, 12, 12, 160)  179200      activation_149[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_150 (BatchN (None, 12, 12, 160)  480         conv2d_150[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_150 (Activation)     (None, 12, 12, 160)  0           batch_normalization_150[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_146 (Conv2D)             (None, 12, 12, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_151 (Conv2D)             (None, 12, 12, 160)  179200      activation_150[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_146 (BatchN (None, 12, 12, 160)  480         conv2d_146[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_151 (BatchN (None, 12, 12, 160)  480         conv2d_151[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_146 (Activation)     (None, 12, 12, 160)  0           batch_normalization_146[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_151 (Activation)     (None, 12, 12, 160)  0           batch_normalization_151[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_147 (Conv2D)             (None, 12, 12, 160)  179200      activation_146[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_152 (Conv2D)             (None, 12, 12, 160)  179200      activation_151[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_147 (BatchN (None, 12, 12, 160)  480         conv2d_147[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_152 (BatchN (None, 12, 12, 160)  480         conv2d_152[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_147 (Activation)     (None, 12, 12, 160)  0           batch_normalization_147[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_152 (Activation)     (None, 12, 12, 160)  0           batch_normalization_152[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_15 (AveragePo (None, 12, 12, 768)  0           mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_145 (Conv2D)             (None, 12, 12, 192)  147456      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_148 (Conv2D)             (None, 12, 12, 192)  215040      activation_147[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_153 (Conv2D)             (None, 12, 12, 192)  215040      activation_152[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_154 (Conv2D)             (None, 12, 12, 192)  147456      average_pooling2d_15[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_145 (BatchN (None, 12, 12, 192)  576         conv2d_145[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_148 (BatchN (None, 12, 12, 192)  576         conv2d_148[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_153 (BatchN (None, 12, 12, 192)  576         conv2d_153[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_154 (BatchN (None, 12, 12, 192)  576         conv2d_154[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_145 (Activation)     (None, 12, 12, 192)  0           batch_normalization_145[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_148 (Activation)     (None, 12, 12, 192)  0           batch_normalization_148[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_153 (Activation)     (None, 12, 12, 192)  0           batch_normalization_153[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_154 (Activation)     (None, 12, 12, 192)  0           batch_normalization_154[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)            (None, 12, 12, 768)  0           activation_145[0][0]             \n",
      "                                                                 activation_148[0][0]             \n",
      "                                                                 activation_153[0][0]             \n",
      "                                                                 activation_154[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_159 (Conv2D)             (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_159 (BatchN (None, 12, 12, 192)  576         conv2d_159[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_159 (Activation)     (None, 12, 12, 192)  0           batch_normalization_159[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_160 (Conv2D)             (None, 12, 12, 192)  258048      activation_159[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_160 (BatchN (None, 12, 12, 192)  576         conv2d_160[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_160 (Activation)     (None, 12, 12, 192)  0           batch_normalization_160[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_156 (Conv2D)             (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_161 (Conv2D)             (None, 12, 12, 192)  258048      activation_160[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_156 (BatchN (None, 12, 12, 192)  576         conv2d_156[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_161 (BatchN (None, 12, 12, 192)  576         conv2d_161[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_156 (Activation)     (None, 12, 12, 192)  0           batch_normalization_156[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_161 (Activation)     (None, 12, 12, 192)  0           batch_normalization_161[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_157 (Conv2D)             (None, 12, 12, 192)  258048      activation_156[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_162 (Conv2D)             (None, 12, 12, 192)  258048      activation_161[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_157 (BatchN (None, 12, 12, 192)  576         conv2d_157[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_162 (BatchN (None, 12, 12, 192)  576         conv2d_162[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_157 (Activation)     (None, 12, 12, 192)  0           batch_normalization_157[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_162 (Activation)     (None, 12, 12, 192)  0           batch_normalization_162[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_16 (AveragePo (None, 12, 12, 768)  0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_155 (Conv2D)             (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_158 (Conv2D)             (None, 12, 12, 192)  258048      activation_157[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_163 (Conv2D)             (None, 12, 12, 192)  258048      activation_162[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_164 (Conv2D)             (None, 12, 12, 192)  147456      average_pooling2d_16[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_155 (BatchN (None, 12, 12, 192)  576         conv2d_155[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_158 (BatchN (None, 12, 12, 192)  576         conv2d_158[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_163 (BatchN (None, 12, 12, 192)  576         conv2d_163[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_164 (BatchN (None, 12, 12, 192)  576         conv2d_164[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_155 (Activation)     (None, 12, 12, 192)  0           batch_normalization_155[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_158 (Activation)     (None, 12, 12, 192)  0           batch_normalization_158[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_163 (Activation)     (None, 12, 12, 192)  0           batch_normalization_163[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_164 (Activation)     (None, 12, 12, 192)  0           batch_normalization_164[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)            (None, 12, 12, 768)  0           activation_155[0][0]             \n",
      "                                                                 activation_158[0][0]             \n",
      "                                                                 activation_163[0][0]             \n",
      "                                                                 activation_164[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_167 (Conv2D)             (None, 12, 12, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_167 (BatchN (None, 12, 12, 192)  576         conv2d_167[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_167 (Activation)     (None, 12, 12, 192)  0           batch_normalization_167[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_168 (Conv2D)             (None, 12, 12, 192)  258048      activation_167[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_168 (BatchN (None, 12, 12, 192)  576         conv2d_168[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_168 (Activation)     (None, 12, 12, 192)  0           batch_normalization_168[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_165 (Conv2D)             (None, 12, 12, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_169 (Conv2D)             (None, 12, 12, 192)  258048      activation_168[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_165 (BatchN (None, 12, 12, 192)  576         conv2d_165[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_169 (BatchN (None, 12, 12, 192)  576         conv2d_169[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_165 (Activation)     (None, 12, 12, 192)  0           batch_normalization_165[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_169 (Activation)     (None, 12, 12, 192)  0           batch_normalization_169[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_166 (Conv2D)             (None, 5, 5, 320)    552960      activation_165[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_170 (Conv2D)             (None, 5, 5, 192)    331776      activation_169[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_166 (BatchN (None, 5, 5, 320)    960         conv2d_166[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_170 (BatchN (None, 5, 5, 192)    576         conv2d_170[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_166 (Activation)     (None, 5, 5, 320)    0           batch_normalization_166[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_170 (Activation)     (None, 5, 5, 192)    0           batch_normalization_170[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 5, 5, 768)    0           mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)            (None, 5, 5, 1280)   0           activation_166[0][0]             \n",
      "                                                                 activation_170[0][0]             \n",
      "                                                                 max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_175 (Conv2D)             (None, 5, 5, 448)    573440      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_175 (BatchN (None, 5, 5, 448)    1344        conv2d_175[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_175 (Activation)     (None, 5, 5, 448)    0           batch_normalization_175[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_172 (Conv2D)             (None, 5, 5, 384)    491520      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_176 (Conv2D)             (None, 5, 5, 384)    1548288     activation_175[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_172 (BatchN (None, 5, 5, 384)    1152        conv2d_172[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_176 (BatchN (None, 5, 5, 384)    1152        conv2d_176[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_172 (Activation)     (None, 5, 5, 384)    0           batch_normalization_172[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_176 (Activation)     (None, 5, 5, 384)    0           batch_normalization_176[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_173 (Conv2D)             (None, 5, 5, 384)    442368      activation_172[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_174 (Conv2D)             (None, 5, 5, 384)    442368      activation_172[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_177 (Conv2D)             (None, 5, 5, 384)    442368      activation_176[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_178 (Conv2D)             (None, 5, 5, 384)    442368      activation_176[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_17 (AveragePo (None, 5, 5, 1280)   0           mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_171 (Conv2D)             (None, 5, 5, 320)    409600      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_173 (BatchN (None, 5, 5, 384)    1152        conv2d_173[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_174 (BatchN (None, 5, 5, 384)    1152        conv2d_174[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_177 (BatchN (None, 5, 5, 384)    1152        conv2d_177[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_178 (BatchN (None, 5, 5, 384)    1152        conv2d_178[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_179 (Conv2D)             (None, 5, 5, 192)    245760      average_pooling2d_17[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_171 (BatchN (None, 5, 5, 320)    960         conv2d_171[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_173 (Activation)     (None, 5, 5, 384)    0           batch_normalization_173[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_174 (Activation)     (None, 5, 5, 384)    0           batch_normalization_174[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_177 (Activation)     (None, 5, 5, 384)    0           batch_normalization_177[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_178 (Activation)     (None, 5, 5, 384)    0           batch_normalization_178[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_179 (BatchN (None, 5, 5, 192)    576         conv2d_179[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_171 (Activation)     (None, 5, 5, 320)    0           batch_normalization_171[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)          (None, 5, 5, 768)    0           activation_173[0][0]             \n",
      "                                                                 activation_174[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 5, 5, 768)    0           activation_177[0][0]             \n",
      "                                                                 activation_178[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_179 (Activation)     (None, 5, 5, 192)    0           batch_normalization_179[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)            (None, 5, 5, 2048)   0           activation_171[0][0]             \n",
      "                                                                 mixed9_0[0][0]                   \n",
      "                                                                 concatenate_3[0][0]              \n",
      "                                                                 activation_179[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_184 (Conv2D)             (None, 5, 5, 448)    917504      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_184 (BatchN (None, 5, 5, 448)    1344        conv2d_184[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_184 (Activation)     (None, 5, 5, 448)    0           batch_normalization_184[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_181 (Conv2D)             (None, 5, 5, 384)    786432      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_185 (Conv2D)             (None, 5, 5, 384)    1548288     activation_184[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_181 (BatchN (None, 5, 5, 384)    1152        conv2d_181[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_185 (BatchN (None, 5, 5, 384)    1152        conv2d_185[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_181 (Activation)     (None, 5, 5, 384)    0           batch_normalization_181[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_185 (Activation)     (None, 5, 5, 384)    0           batch_normalization_185[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_182 (Conv2D)             (None, 5, 5, 384)    442368      activation_181[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_183 (Conv2D)             (None, 5, 5, 384)    442368      activation_181[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_186 (Conv2D)             (None, 5, 5, 384)    442368      activation_185[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_187 (Conv2D)             (None, 5, 5, 384)    442368      activation_185[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_18 (AveragePo (None, 5, 5, 2048)   0           mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_180 (Conv2D)             (None, 5, 5, 320)    655360      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_182 (BatchN (None, 5, 5, 384)    1152        conv2d_182[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_183 (BatchN (None, 5, 5, 384)    1152        conv2d_183[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_186 (BatchN (None, 5, 5, 384)    1152        conv2d_186[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_187 (BatchN (None, 5, 5, 384)    1152        conv2d_187[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_188 (Conv2D)             (None, 5, 5, 192)    393216      average_pooling2d_18[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_180 (BatchN (None, 5, 5, 320)    960         conv2d_180[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_182 (Activation)     (None, 5, 5, 384)    0           batch_normalization_182[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_183 (Activation)     (None, 5, 5, 384)    0           batch_normalization_183[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_186 (Activation)     (None, 5, 5, 384)    0           batch_normalization_186[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_187 (Activation)     (None, 5, 5, 384)    0           batch_normalization_187[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_188 (BatchN (None, 5, 5, 192)    576         conv2d_188[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_180 (Activation)     (None, 5, 5, 320)    0           batch_normalization_180[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)          (None, 5, 5, 768)    0           activation_182[0][0]             \n",
      "                                                                 activation_183[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 5, 5, 768)    0           activation_186[0][0]             \n",
      "                                                                 activation_187[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_188 (Activation)     (None, 5, 5, 192)    0           batch_normalization_188[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)           (None, 5, 5, 2048)   0           activation_180[0][0]             \n",
      "                                                                 mixed9_1[0][0]                   \n",
      "                                                                 concatenate_4[0][0]              \n",
      "                                                                 activation_188[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 2048)         0           mixed10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Dense)             (None, 3)            6147        avg_pool[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 21,808,931\n",
      "Trainable params: 21,774,499\n",
      "Non-trainable params: 34,432\n",
      "__________________________________________________________________________________________________\n",
      "Using Enhanced Data Generation\n",
      "Found 951 images belonging to 3 classes.\n",
      "Found 774 images belonging to 3 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON Mapping for the model classes saved to  Machine Learning InceptionV3\\json\\model_class.json\n",
      "Number of experiments (Epochs) :  200\n",
      "Epoch 1/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 1.1415 - acc: 0.3270\n",
      "Epoch 00001: val_acc improved from -inf to 0.33073, saving model to Machine Learning InceptionV3\\models\\model_ex-001_acc-0.330729.h5\n",
      "29/29 [==============================] - 41s 1s/step - loss: 1.1395 - acc: 0.3265 - val_loss: 1.0992 - val_acc: 0.3307\n",
      "Epoch 2/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 1.0765 - acc: 0.4222\n",
      "Epoch 00002: val_acc did not improve\n",
      "29/29 [==============================] - 17s 592ms/step - loss: 1.0749 - acc: 0.4218 - val_loss: 1.1003 - val_acc: 0.3307\n",
      "Epoch 3/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 1.0208 - acc: 0.4679\n",
      "Epoch 00003: val_acc did not improve\n",
      "29/29 [==============================] - 17s 598ms/step - loss: 1.0207 - acc: 0.4679 - val_loss: 1.1037 - val_acc: 0.3307\n",
      "Epoch 4/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.9570 - acc: 0.5056\n",
      "Epoch 00004: val_acc did not improve\n",
      "29/29 [==============================] - 17s 581ms/step - loss: 0.9583 - acc: 0.5017 - val_loss: 1.1123 - val_acc: 0.3307\n",
      "Epoch 5/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.9221 - acc: 0.5528\n",
      "Epoch 00005: val_acc did not improve\n",
      "29/29 [==============================] - 17s 587ms/step - loss: 0.9203 - acc: 0.5553 - val_loss: 1.1228 - val_acc: 0.3307\n",
      "Epoch 6/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.8597 - acc: 0.6420\n",
      "Epoch 00006: val_acc did not improve\n",
      "29/29 [==============================] - 17s 579ms/step - loss: 0.8600 - acc: 0.6447 - val_loss: 1.1394 - val_acc: 0.3307\n",
      "Epoch 7/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.8170 - acc: 0.7007\n",
      "Epoch 00007: val_acc did not improve\n",
      "29/29 [==============================] - 17s 570ms/step - loss: 0.8139 - acc: 0.7057 - val_loss: 1.1459 - val_acc: 0.3307\n",
      "Epoch 8/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.7849 - acc: 0.7244\n",
      "Epoch 00008: val_acc improved from 0.33073 to 0.33203, saving model to Machine Learning InceptionV3\\models\\model_ex-008_acc-0.332031.h5\n",
      "29/29 [==============================] - 17s 575ms/step - loss: 0.7858 - acc: 0.7220 - val_loss: 1.1762 - val_acc: 0.3320\n",
      "Epoch 9/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.7534 - acc: 0.7467\n",
      "Epoch 00009: val_acc did not improve\n",
      "29/29 [==============================] - 16s 564ms/step - loss: 0.7521 - acc: 0.7500 - val_loss: 1.2296 - val_acc: 0.3255\n",
      "Epoch 10/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.7098 - acc: 0.7905\n",
      "Epoch 00010: val_acc did not improve\n",
      "29/29 [==============================] - 16s 548ms/step - loss: 0.7092 - acc: 0.7869 - val_loss: 1.3042 - val_acc: 0.3294\n",
      "Epoch 11/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.7271 - acc: 0.7533\n",
      "Epoch 00011: val_acc did not improve\n",
      "29/29 [==============================] - 16s 559ms/step - loss: 0.7254 - acc: 0.7522 - val_loss: 1.3783 - val_acc: 0.3307\n",
      "Epoch 12/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.6585 - acc: 0.7941\n",
      "Epoch 00012: val_acc did not improve\n",
      "29/29 [==============================] - 16s 541ms/step - loss: 0.6584 - acc: 0.7915 - val_loss: 1.3739 - val_acc: 0.3307\n",
      "Epoch 13/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.6487 - acc: 0.8058\n",
      "Epoch 00013: val_acc improved from 0.33203 to 0.33724, saving model to Machine Learning InceptionV3\\models\\model_ex-013_acc-0.337240.h5\n",
      "29/29 [==============================] - 16s 555ms/step - loss: 0.6442 - acc: 0.8093 - val_loss: 1.2602 - val_acc: 0.3372\n",
      "Epoch 14/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.6210 - acc: 0.8026\n",
      "Epoch 00014: val_acc improved from 0.33724 to 0.38672, saving model to Machine Learning InceptionV3\\models\\model_ex-014_acc-0.386719.h5\n",
      "29/29 [==============================] - 15s 532ms/step - loss: 0.6225 - acc: 0.8008 - val_loss: 1.1292 - val_acc: 0.3867\n",
      "Epoch 15/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.5923 - acc: 0.8058\n",
      "Epoch 00015: val_acc improved from 0.38672 to 0.45573, saving model to Machine Learning InceptionV3\\models\\model_ex-015_acc-0.455729.h5\n",
      "29/29 [==============================] - 16s 540ms/step - loss: 0.5917 - acc: 0.8050 - val_loss: 1.0051 - val_acc: 0.4557\n",
      "Epoch 16/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.5989 - acc: 0.7999\n",
      "Epoch 00016: val_acc improved from 0.45573 to 0.55859, saving model to Machine Learning InceptionV3\\models\\model_ex-016_acc-0.558594.h5\n",
      "29/29 [==============================] - 15s 534ms/step - loss: 0.5970 - acc: 0.8004 - val_loss: 0.8979 - val_acc: 0.5586\n",
      "Epoch 17/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.5632 - acc: 0.8273\n",
      "Epoch 00017: val_acc improved from 0.55859 to 0.60807, saving model to Machine Learning InceptionV3\\models\\model_ex-017_acc-0.608073.h5\n",
      "29/29 [==============================] - 15s 518ms/step - loss: 0.5698 - acc: 0.8225 - val_loss: 0.8230 - val_acc: 0.6081\n",
      "Epoch 18/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.5437 - acc: 0.8225\n",
      "Epoch 00018: val_acc improved from 0.60807 to 0.62240, saving model to Machine Learning InceptionV3\\models\\model_ex-018_acc-0.622396.h5\n",
      "29/29 [==============================] - 15s 521ms/step - loss: 0.5471 - acc: 0.8200 - val_loss: 0.7624 - val_acc: 0.6224\n",
      "Epoch 19/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.5290 - acc: 0.8367\n",
      "Epoch 00019: val_acc improved from 0.62240 to 0.66016, saving model to Machine Learning InceptionV3\\models\\model_ex-019_acc-0.660156.h5\n",
      "29/29 [==============================] - 15s 514ms/step - loss: 0.5251 - acc: 0.8369 - val_loss: 0.7174 - val_acc: 0.6602\n",
      "Epoch 20/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.5175 - acc: 0.8371\n",
      "Epoch 00020: val_acc improved from 0.66016 to 0.68750, saving model to Machine Learning InceptionV3\\models\\model_ex-020_acc-0.687500.h5\n",
      "29/29 [==============================] - 16s 540ms/step - loss: 0.5141 - acc: 0.8396 - val_loss: 0.6731 - val_acc: 0.6875\n",
      "Epoch 21/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.5177 - acc: 0.8141\n",
      "Epoch 00021: val_acc improved from 0.68750 to 0.70703, saving model to Machine Learning InceptionV3\\models\\model_ex-021_acc-0.707031.h5\n",
      "29/29 [==============================] - 15s 519ms/step - loss: 0.5167 - acc: 0.8151 - val_loss: 0.6499 - val_acc: 0.7070\n",
      "Epoch 22/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.4884 - acc: 0.8359\n",
      "Epoch 00022: val_acc improved from 0.70703 to 0.73438, saving model to Machine Learning InceptionV3\\models\\model_ex-022_acc-0.734375.h5\n",
      "29/29 [==============================] - 14s 498ms/step - loss: 0.4850 - acc: 0.8384 - val_loss: 0.6248 - val_acc: 0.7344\n",
      "Epoch 23/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.4715 - acc: 0.8527\n",
      "Epoch 00023: val_acc improved from 0.73438 to 0.74740, saving model to Machine Learning InceptionV3\\models\\model_ex-023_acc-0.747396.h5\n",
      "29/29 [==============================] - 14s 499ms/step - loss: 0.4714 - acc: 0.8534 - val_loss: 0.6068 - val_acc: 0.7474\n",
      "Epoch 24/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.4637 - acc: 0.8297\n",
      "Epoch 00024: val_acc improved from 0.74740 to 0.76823, saving model to Machine Learning InceptionV3\\models\\model_ex-024_acc-0.768229.h5\n",
      "29/29 [==============================] - 16s 535ms/step - loss: 0.4651 - acc: 0.8280 - val_loss: 0.5943 - val_acc: 0.7682\n",
      "Epoch 25/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.4699 - acc: 0.8326\n",
      "Epoch 00025: val_acc did not improve\n",
      "29/29 [==============================] - 15s 516ms/step - loss: 0.4672 - acc: 0.8341 - val_loss: 0.5886 - val_acc: 0.7643\n",
      "Epoch 26/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.4429 - acc: 0.8465\n",
      "Epoch 00026: val_acc improved from 0.76823 to 0.77604, saving model to Machine Learning InceptionV3\\models\\model_ex-026_acc-0.776042.h5\n",
      "29/29 [==============================] - 15s 522ms/step - loss: 0.4477 - acc: 0.8431 - val_loss: 0.5762 - val_acc: 0.7760\n",
      "Epoch 27/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.4436 - acc: 0.8360\n",
      "Epoch 00027: val_acc improved from 0.77604 to 0.77995, saving model to Machine Learning InceptionV3\\models\\model_ex-027_acc-0.779948.h5\n",
      "29/29 [==============================] - 15s 504ms/step - loss: 0.4427 - acc: 0.8363 - val_loss: 0.5679 - val_acc: 0.7799\n",
      "Epoch 28/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.4408 - acc: 0.8503\n",
      "Epoch 00028: val_acc did not improve\n",
      "29/29 [==============================] - 14s 488ms/step - loss: 0.4386 - acc: 0.8522 - val_loss: 0.5674 - val_acc: 0.7760\n",
      "Epoch 29/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.4256 - acc: 0.8480\n",
      "Epoch 00029: val_acc improved from 0.77995 to 0.78125, saving model to Machine Learning InceptionV3\\models\\model_ex-029_acc-0.781250.h5\n",
      "29/29 [==============================] - 13s 465ms/step - loss: 0.4237 - acc: 0.8489 - val_loss: 0.5567 - val_acc: 0.7812\n",
      "Epoch 30/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.3996 - acc: 0.8645\n",
      "Epoch 00030: val_acc improved from 0.78125 to 0.78646, saving model to Machine Learning InceptionV3\\models\\model_ex-030_acc-0.786458.h5\n",
      "29/29 [==============================] - 14s 484ms/step - loss: 0.4013 - acc: 0.8627 - val_loss: 0.5530 - val_acc: 0.7865\n",
      "Epoch 31/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.4075 - acc: 0.8454\n",
      "Epoch 00031: val_acc improved from 0.78646 to 0.79688, saving model to Machine Learning InceptionV3\\models\\model_ex-031_acc-0.796875.h5\n",
      "29/29 [==============================] - 13s 462ms/step - loss: 0.4142 - acc: 0.8431 - val_loss: 0.5407 - val_acc: 0.7969\n",
      "Epoch 32/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.4115 - acc: 0.8563\n",
      "Epoch 00032: val_acc improved from 0.79688 to 0.79818, saving model to Machine Learning InceptionV3\\models\\model_ex-032_acc-0.798177.h5\n",
      "29/29 [==============================] - 19s 653ms/step - loss: 0.4098 - acc: 0.8569 - val_loss: 0.5362 - val_acc: 0.7982\n",
      "Epoch 33/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.3965 - acc: 0.8482\n",
      "Epoch 00033: val_acc improved from 0.79818 to 0.80339, saving model to Machine Learning InceptionV3\\models\\model_ex-033_acc-0.803385.h5\n",
      "29/29 [==============================] - 17s 601ms/step - loss: 0.3956 - acc: 0.8489 - val_loss: 0.5283 - val_acc: 0.8034\n",
      "Epoch 34/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.3969 - acc: 0.8594\n",
      "Epoch 00034: val_acc improved from 0.80339 to 0.80859, saving model to Machine Learning InceptionV3\\models\\model_ex-034_acc-0.808594.h5\n",
      "29/29 [==============================] - 17s 598ms/step - loss: 0.3939 - acc: 0.8610 - val_loss: 0.5226 - val_acc: 0.8086\n",
      "Epoch 35/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.3779 - acc: 0.8674\n",
      "Epoch 00035: val_acc improved from 0.80859 to 0.81120, saving model to Machine Learning InceptionV3\\models\\model_ex-035_acc-0.811198.h5\n",
      "29/29 [==============================] - 17s 593ms/step - loss: 0.3755 - acc: 0.8698 - val_loss: 0.5206 - val_acc: 0.8112\n",
      "Epoch 36/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.3817 - acc: 0.8749\n",
      "Epoch 00036: val_acc did not improve\n",
      "29/29 [==============================] - 17s 581ms/step - loss: 0.3820 - acc: 0.8727 - val_loss: 0.5166 - val_acc: 0.8008\n",
      "Epoch 37/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.3853 - acc: 0.8585\n",
      "Epoch 00037: val_acc did not improve\n",
      "29/29 [==============================] - 16s 562ms/step - loss: 0.3840 - acc: 0.8580 - val_loss: 0.5245 - val_acc: 0.7995\n",
      "Epoch 38/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.3571 - acc: 0.8641\n",
      "Epoch 00038: val_acc improved from 0.81120 to 0.81250, saving model to Machine Learning InceptionV3\\models\\model_ex-038_acc-0.812500.h5\n",
      "29/29 [==============================] - 16s 565ms/step - loss: 0.3571 - acc: 0.8634 - val_loss: 0.5146 - val_acc: 0.8125\n",
      "Epoch 39/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.3508 - acc: 0.8793\n",
      "Epoch 00039: val_acc did not improve\n",
      "29/29 [==============================] - 16s 554ms/step - loss: 0.3484 - acc: 0.8813 - val_loss: 0.5048 - val_acc: 0.8125\n",
      "Epoch 40/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.3714 - acc: 0.8549\n",
      "Epoch 00040: val_acc did not improve\n",
      "29/29 [==============================] - 16s 545ms/step - loss: 0.3754 - acc: 0.8524 - val_loss: 0.5050 - val_acc: 0.8125\n",
      "Epoch 41/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.3393 - acc: 0.8819\n",
      "Epoch 00041: val_acc improved from 0.81250 to 0.81771, saving model to Machine Learning InceptionV3\\models\\model_ex-041_acc-0.817708.h5\n",
      "29/29 [==============================] - 16s 568ms/step - loss: 0.3401 - acc: 0.8785 - val_loss: 0.5008 - val_acc: 0.8177\n",
      "Epoch 42/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.3378 - acc: 0.8828\n",
      "Epoch 00042: val_acc did not improve\n",
      "29/29 [==============================] - 16s 535ms/step - loss: 0.3386 - acc: 0.8814 - val_loss: 0.4975 - val_acc: 0.8151\n",
      "Epoch 43/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.3371 - acc: 0.8663\n",
      "Epoch 00043: val_acc did not improve\n",
      "29/29 [==============================] - 15s 531ms/step - loss: 0.3342 - acc: 0.8698 - val_loss: 0.4967 - val_acc: 0.8125\n",
      "Epoch 44/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.3439 - acc: 0.8739\n",
      "Epoch 00044: val_acc did not improve\n",
      "29/29 [==============================] - 15s 527ms/step - loss: 0.3435 - acc: 0.8739 - val_loss: 0.4925 - val_acc: 0.8177\n",
      "Epoch 45/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.3245 - acc: 0.8842\n",
      "Epoch 00045: val_acc improved from 0.81771 to 0.81901, saving model to Machine Learning InceptionV3\\models\\model_ex-045_acc-0.819010.h5\n",
      "29/29 [==============================] - 15s 534ms/step - loss: 0.3280 - acc: 0.8806 - val_loss: 0.4895 - val_acc: 0.8190\n",
      "Epoch 46/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.3258 - acc: 0.8783\n",
      "Epoch 00046: val_acc did not improve\n",
      "29/29 [==============================] - 15s 531ms/step - loss: 0.3247 - acc: 0.8782 - val_loss: 0.4858 - val_acc: 0.8164\n",
      "Epoch 47/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.3349 - acc: 0.8704\n",
      "Epoch 00047: val_acc did not improve\n",
      "29/29 [==============================] - 15s 529ms/step - loss: 0.3306 - acc: 0.8717 - val_loss: 0.4824 - val_acc: 0.8112\n",
      "Epoch 48/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.3135 - acc: 0.8750\n",
      "Epoch 00048: val_acc did not improve\n",
      "29/29 [==============================] - 16s 545ms/step - loss: 0.3156 - acc: 0.8739 - val_loss: 0.4794 - val_acc: 0.8138\n",
      "Epoch 49/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.3365 - acc: 0.8657\n",
      "Epoch 00049: val_acc did not improve\n",
      "29/29 [==============================] - 15s 509ms/step - loss: 0.3357 - acc: 0.8671 - val_loss: 0.4801 - val_acc: 0.8138\n",
      "Epoch 50/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2856 - acc: 0.9007\n",
      "Epoch 00050: val_acc did not improve\n",
      "29/29 [==============================] - 15s 509ms/step - loss: 0.2905 - acc: 0.8955 - val_loss: 0.4774 - val_acc: 0.8177\n",
      "Epoch 51/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.3219 - acc: 0.8770\n",
      "Epoch 00051: val_acc did not improve\n",
      "29/29 [==============================] - 15s 503ms/step - loss: 0.3211 - acc: 0.8759 - val_loss: 0.4811 - val_acc: 0.8138\n",
      "Epoch 52/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.3040 - acc: 0.8904\n",
      "Epoch 00052: val_acc did not improve\n",
      "29/29 [==============================] - 14s 487ms/step - loss: 0.3035 - acc: 0.8899 - val_loss: 0.4883 - val_acc: 0.8138\n",
      "Epoch 53/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.3059 - acc: 0.8846\n",
      "Epoch 00053: val_acc did not improve\n",
      "29/29 [==============================] - 14s 480ms/step - loss: 0.3045 - acc: 0.8843 - val_loss: 0.4868 - val_acc: 0.8138\n",
      "Epoch 54/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.3056 - acc: 0.8782\n",
      "Epoch 00054: val_acc improved from 0.81901 to 0.82031, saving model to Machine Learning InceptionV3\\models\\model_ex-054_acc-0.820312.h5\n",
      "29/29 [==============================] - 14s 481ms/step - loss: 0.3061 - acc: 0.8780 - val_loss: 0.4759 - val_acc: 0.8203\n",
      "Epoch 55/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2926 - acc: 0.8929\n",
      "Epoch 00055: val_acc did not improve\n",
      "29/29 [==============================] - 13s 457ms/step - loss: 0.2956 - acc: 0.8879 - val_loss: 0.4723 - val_acc: 0.8151\n",
      "Epoch 56/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2827 - acc: 0.8849\n",
      "Epoch 00056: val_acc did not improve\n",
      "29/29 [==============================] - 13s 456ms/step - loss: 0.2869 - acc: 0.8834 - val_loss: 0.4693 - val_acc: 0.8164\n",
      "Epoch 57/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.3163 - acc: 0.8733\n",
      "Epoch 00057: val_acc improved from 0.82031 to 0.82422, saving model to Machine Learning InceptionV3\\models\\model_ex-057_acc-0.824219.h5\n",
      "29/29 [==============================] - 13s 465ms/step - loss: 0.3145 - acc: 0.8723 - val_loss: 0.4618 - val_acc: 0.8242\n",
      "Epoch 58/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2691 - acc: 0.9069\n",
      "Epoch 00058: val_acc improved from 0.82422 to 0.82812, saving model to Machine Learning InceptionV3\\models\\model_ex-058_acc-0.828125.h5\n",
      "29/29 [==============================] - 13s 463ms/step - loss: 0.2671 - acc: 0.9080 - val_loss: 0.4603 - val_acc: 0.8281\n",
      "Epoch 59/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2994 - acc: 0.8813\n",
      "Epoch 00059: val_acc did not improve\n",
      "29/29 [==============================] - 14s 476ms/step - loss: 0.2947 - acc: 0.8853 - val_loss: 0.4583 - val_acc: 0.8281\n",
      "Epoch 60/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2809 - acc: 0.8947\n",
      "Epoch 00060: val_acc did not improve\n",
      "29/29 [==============================] - 13s 458ms/step - loss: 0.2826 - acc: 0.8940 - val_loss: 0.4625 - val_acc: 0.8190\n",
      "Epoch 61/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2735 - acc: 0.9013\n",
      "Epoch 00061: val_acc did not improve\n",
      "29/29 [==============================] - 13s 446ms/step - loss: 0.2731 - acc: 0.9037 - val_loss: 0.4709 - val_acc: 0.8229\n",
      "Epoch 62/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2971 - acc: 0.8813\n",
      "Epoch 00062: val_acc did not improve\n",
      "29/29 [==============================] - 17s 594ms/step - loss: 0.2936 - acc: 0.8832 - val_loss: 0.4668 - val_acc: 0.8255\n",
      "Epoch 63/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2754 - acc: 0.8917\n",
      "Epoch 00063: val_acc did not improve\n",
      "29/29 [==============================] - 17s 581ms/step - loss: 0.2729 - acc: 0.8940 - val_loss: 0.4631 - val_acc: 0.8281\n",
      "Epoch 64/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2686 - acc: 0.8982\n",
      "Epoch 00064: val_acc did not improve\n",
      "29/29 [==============================] - 17s 581ms/step - loss: 0.2684 - acc: 0.8975 - val_loss: 0.4637 - val_acc: 0.8216\n",
      "Epoch 65/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2655 - acc: 0.8893\n",
      "Epoch 00065: val_acc did not improve\n",
      "29/29 [==============================] - 17s 585ms/step - loss: 0.2636 - acc: 0.8921 - val_loss: 0.4562 - val_acc: 0.8268\n",
      "Epoch 66/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2901 - acc: 0.8817\n",
      "Epoch 00066: val_acc did not improve\n",
      "29/29 [==============================] - 17s 570ms/step - loss: 0.2885 - acc: 0.8837 - val_loss: 0.4577 - val_acc: 0.8242\n",
      "Epoch 67/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2701 - acc: 0.9062\n",
      "Epoch 00067: val_acc did not improve\n",
      "29/29 [==============================] - 16s 562ms/step - loss: 0.2754 - acc: 0.9030 - val_loss: 0.4665 - val_acc: 0.8190\n",
      "Epoch 68/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2446 - acc: 0.9141\n",
      "Epoch 00068: val_acc did not improve\n",
      "29/29 [==============================] - 16s 562ms/step - loss: 0.2457 - acc: 0.9116 - val_loss: 0.4650 - val_acc: 0.8164\n",
      "Epoch 69/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2831 - acc: 0.8874\n",
      "Epoch 00069: val_acc did not improve\n",
      "29/29 [==============================] - 16s 547ms/step - loss: 0.2780 - acc: 0.8913 - val_loss: 0.4656 - val_acc: 0.8216\n",
      "Epoch 70/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2841 - acc: 0.8909\n",
      "Epoch 00070: val_acc did not improve\n",
      "29/29 [==============================] - 16s 547ms/step - loss: 0.2835 - acc: 0.8914 - val_loss: 0.4597 - val_acc: 0.8216\n",
      "Epoch 71/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2684 - acc: 0.8914\n",
      "Epoch 00071: val_acc did not improve\n",
      "29/29 [==============================] - 16s 537ms/step - loss: 0.2658 - acc: 0.8930 - val_loss: 0.4535 - val_acc: 0.8216\n",
      "Epoch 72/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2623 - acc: 0.8980\n",
      "Epoch 00072: val_acc did not improve\n",
      "29/29 [==============================] - 15s 534ms/step - loss: 0.2586 - acc: 0.8993 - val_loss: 0.4513 - val_acc: 0.8281\n",
      "Epoch 73/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2640 - acc: 0.9009\n",
      "Epoch 00073: val_acc did not improve\n",
      "29/29 [==============================] - 15s 528ms/step - loss: 0.2638 - acc: 0.9011 - val_loss: 0.4574 - val_acc: 0.8229\n",
      "Epoch 74/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2500 - acc: 0.9080\n",
      "Epoch 00074: val_acc did not improve\n",
      "29/29 [==============================] - 15s 528ms/step - loss: 0.2481 - acc: 0.9090 - val_loss: 0.4689 - val_acc: 0.8164\n",
      "Epoch 75/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2488 - acc: 0.9141\n",
      "Epoch 00075: val_acc did not improve\n",
      "29/29 [==============================] - 16s 539ms/step - loss: 0.2511 - acc: 0.9138 - val_loss: 0.4573 - val_acc: 0.8164\n",
      "Epoch 76/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2572 - acc: 0.8933\n",
      "Epoch 00076: val_acc did not improve\n",
      "29/29 [==============================] - 15s 527ms/step - loss: 0.2536 - acc: 0.8960 - val_loss: 0.4564 - val_acc: 0.8229\n",
      "Epoch 77/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2358 - acc: 0.9085\n",
      "Epoch 00077: val_acc did not improve\n",
      "29/29 [==============================] - 15s 518ms/step - loss: 0.2422 - acc: 0.9051 - val_loss: 0.4527 - val_acc: 0.8190\n",
      "Epoch 78/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2392 - acc: 0.9105\n",
      "Epoch 00078: val_acc did not improve\n",
      "29/29 [==============================] - 15s 512ms/step - loss: 0.2390 - acc: 0.9104 - val_loss: 0.4516 - val_acc: 0.8216\n",
      "Epoch 79/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2548 - acc: 0.8973\n",
      "Epoch 00079: val_acc did not improve\n",
      "29/29 [==============================] - 15s 512ms/step - loss: 0.2520 - acc: 0.8987 - val_loss: 0.4464 - val_acc: 0.8242\n",
      "Epoch 80/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2561 - acc: 0.9054\n",
      "Epoch 00080: val_acc did not improve\n",
      "29/29 [==============================] - 15s 510ms/step - loss: 0.2638 - acc: 0.8989 - val_loss: 0.4429 - val_acc: 0.8268\n",
      "Epoch 81/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2647 - acc: 0.8984\n",
      "Epoch 00081: val_acc improved from 0.82812 to 0.83073, saving model to Machine Learning InceptionV3\\models\\model_ex-081_acc-0.830729.h5\n",
      "29/29 [==============================] - 15s 513ms/step - loss: 0.2616 - acc: 0.9019 - val_loss: 0.4434 - val_acc: 0.8307\n",
      "Epoch 82/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2238 - acc: 0.9085\n",
      "Epoch 00082: val_acc did not improve\n",
      "29/29 [==============================] - 14s 479ms/step - loss: 0.2227 - acc: 0.9085 - val_loss: 0.4465 - val_acc: 0.8281\n",
      "Epoch 83/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2548 - acc: 0.8996\n",
      "Epoch 00083: val_acc did not improve\n",
      "29/29 [==============================] - 14s 480ms/step - loss: 0.2509 - acc: 0.9019 - val_loss: 0.4476 - val_acc: 0.8255\n",
      "Epoch 84/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2491 - acc: 0.9028\n",
      "Epoch 00084: val_acc did not improve\n",
      "29/29 [==============================] - 13s 462ms/step - loss: 0.2450 - acc: 0.9051 - val_loss: 0.4420 - val_acc: 0.8294\n",
      "Epoch 85/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2488 - acc: 0.9013\n",
      "Epoch 00085: val_acc improved from 0.83073 to 0.83203, saving model to Machine Learning InceptionV3\\models\\model_ex-085_acc-0.832031.h5\n",
      "29/29 [==============================] - 14s 475ms/step - loss: 0.2483 - acc: 0.9026 - val_loss: 0.4440 - val_acc: 0.8320\n",
      "Epoch 86/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2462 - acc: 0.9072\n",
      "Epoch 00086: val_acc did not improve\n",
      "29/29 [==============================] - 13s 464ms/step - loss: 0.2463 - acc: 0.9061 - val_loss: 0.4424 - val_acc: 0.8242\n",
      "Epoch 87/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2291 - acc: 0.9096\n",
      "Epoch 00087: val_acc did not improve\n",
      "29/29 [==============================] - 13s 457ms/step - loss: 0.2308 - acc: 0.9095 - val_loss: 0.4414 - val_acc: 0.8255\n",
      "Epoch 88/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2372 - acc: 0.9098\n",
      "Epoch 00088: val_acc did not improve\n",
      "29/29 [==============================] - 13s 458ms/step - loss: 0.2386 - acc: 0.9086 - val_loss: 0.4407 - val_acc: 0.8307\n",
      "Epoch 89/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2284 - acc: 0.9177\n",
      "Epoch 00089: val_acc did not improve\n",
      "29/29 [==============================] - 13s 449ms/step - loss: 0.2307 - acc: 0.9151 - val_loss: 0.4385 - val_acc: 0.8281\n",
      "Epoch 90/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2537 - acc: 0.8917\n",
      "Epoch 00090: val_acc did not improve\n",
      "29/29 [==============================] - 13s 450ms/step - loss: 0.2493 - acc: 0.8954 - val_loss: 0.4378 - val_acc: 0.8294\n",
      "Epoch 91/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2541 - acc: 0.9034\n",
      "Epoch 00091: val_acc did not improve\n",
      "29/29 [==============================] - 13s 446ms/step - loss: 0.2533 - acc: 0.9024 - val_loss: 0.4393 - val_acc: 0.8320\n",
      "Epoch 92/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2376 - acc: 0.9098\n",
      "Epoch 00092: val_acc did not improve\n",
      "29/29 [==============================] - 17s 590ms/step - loss: 0.2364 - acc: 0.9097 - val_loss: 0.4410 - val_acc: 0.8255\n",
      "Epoch 93/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2567 - acc: 0.9012\n",
      "Epoch 00093: val_acc did not improve\n",
      "29/29 [==============================] - 17s 600ms/step - loss: 0.2565 - acc: 0.8992 - val_loss: 0.4419 - val_acc: 0.8307\n",
      "Epoch 94/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2628 - acc: 0.8938\n",
      "Epoch 00094: val_acc did not improve\n",
      "29/29 [==============================] - 17s 591ms/step - loss: 0.2591 - acc: 0.8964 - val_loss: 0.4406 - val_acc: 0.8307\n",
      "Epoch 95/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2219 - acc: 0.9128\n",
      "Epoch 00095: val_acc did not improve\n",
      "29/29 [==============================] - 17s 583ms/step - loss: 0.2200 - acc: 0.9136 - val_loss: 0.4430 - val_acc: 0.8242\n",
      "Epoch 96/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2683 - acc: 0.8887\n",
      "Epoch 00096: val_acc did not improve\n",
      "29/29 [==============================] - 17s 587ms/step - loss: 0.2680 - acc: 0.8872 - val_loss: 0.4396 - val_acc: 0.8268\n",
      "Epoch 97/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2534 - acc: 0.8929\n",
      "Epoch 00097: val_acc did not improve\n",
      "29/29 [==============================] - 17s 586ms/step - loss: 0.2531 - acc: 0.8944 - val_loss: 0.4421 - val_acc: 0.8242\n",
      "Epoch 98/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2312 - acc: 0.9085\n",
      "Epoch 00098: val_acc did not improve\n",
      "29/29 [==============================] - 16s 568ms/step - loss: 0.2328 - acc: 0.9085 - val_loss: 0.4410 - val_acc: 0.8307\n",
      "Epoch 99/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2401 - acc: 0.9107\n",
      "Epoch 00099: val_acc did not improve\n",
      "29/29 [==============================] - 17s 571ms/step - loss: 0.2425 - acc: 0.9105 - val_loss: 0.4438 - val_acc: 0.8268\n",
      "Epoch 100/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2344 - acc: 0.9085\n",
      "Epoch 00100: val_acc did not improve\n",
      "29/29 [==============================] - 16s 553ms/step - loss: 0.2337 - acc: 0.9095 - val_loss: 0.4430 - val_acc: 0.8281\n",
      "Epoch 101/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2449 - acc: 0.9016\n",
      "Epoch 00101: val_acc did not improve\n",
      "29/29 [==============================] - 16s 539ms/step - loss: 0.2523 - acc: 0.8974 - val_loss: 0.4416 - val_acc: 0.8255\n",
      "Epoch 102/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2118 - acc: 0.9185\n",
      "Epoch 00102: val_acc did not improve\n",
      "29/29 [==============================] - 15s 532ms/step - loss: 0.2148 - acc: 0.9170 - val_loss: 0.4457 - val_acc: 0.8268\n",
      "Epoch 103/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2524 - acc: 0.8978\n",
      "Epoch 00103: val_acc did not improve\n",
      "29/29 [==============================] - 15s 530ms/step - loss: 0.2486 - acc: 0.9014 - val_loss: 0.4438 - val_acc: 0.8281\n",
      "Epoch 104/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2487 - acc: 0.8987\n",
      "Epoch 00104: val_acc did not improve\n",
      "29/29 [==============================] - 15s 521ms/step - loss: 0.2443 - acc: 0.9011 - val_loss: 0.4441 - val_acc: 0.8281\n",
      "Epoch 105/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2526 - acc: 0.9054\n",
      "Epoch 00105: val_acc did not improve\n",
      "29/29 [==============================] - 15s 517ms/step - loss: 0.2503 - acc: 0.9065 - val_loss: 0.4440 - val_acc: 0.8255\n",
      "Epoch 106/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2690 - acc: 0.8828\n",
      "Epoch 00106: val_acc did not improve\n",
      "29/29 [==============================] - 15s 512ms/step - loss: 0.2661 - acc: 0.8847 - val_loss: 0.4404 - val_acc: 0.8307\n",
      "Epoch 107/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2251 - acc: 0.9248\n",
      "Epoch 00107: val_acc did not improve\n",
      "29/29 [==============================] - 15s 507ms/step - loss: 0.2229 - acc: 0.9241 - val_loss: 0.4430 - val_acc: 0.8268\n",
      "Epoch 108/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2461 - acc: 0.9052\n",
      "Epoch 00108: val_acc did not improve\n",
      "29/29 [==============================] - 14s 496ms/step - loss: 0.2432 - acc: 0.9074 - val_loss: 0.4412 - val_acc: 0.8255\n",
      "Epoch 109/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2441 - acc: 0.8873\n",
      "Epoch 00109: val_acc did not improve\n",
      "29/29 [==============================] - 15s 513ms/step - loss: 0.2444 - acc: 0.8890 - val_loss: 0.4414 - val_acc: 0.8242\n",
      "Epoch 110/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2527 - acc: 0.8969\n",
      "Epoch 00110: val_acc did not improve\n",
      "29/29 [==============================] - 15s 506ms/step - loss: 0.2522 - acc: 0.8972 - val_loss: 0.4396 - val_acc: 0.8281\n",
      "Epoch 111/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2353 - acc: 0.9183\n",
      "Epoch 00111: val_acc did not improve\n",
      "29/29 [==============================] - 14s 479ms/step - loss: 0.2349 - acc: 0.9190 - val_loss: 0.4409 - val_acc: 0.8255\n",
      "Epoch 112/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2422 - acc: 0.9007\n",
      "Epoch 00112: val_acc did not improve\n",
      "29/29 [==============================] - 14s 477ms/step - loss: 0.2434 - acc: 0.9009 - val_loss: 0.4406 - val_acc: 0.8268\n",
      "Epoch 113/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2306 - acc: 0.9132\n",
      "Epoch 00113: val_acc did not improve\n",
      "29/29 [==============================] - 14s 475ms/step - loss: 0.2291 - acc: 0.9151 - val_loss: 0.4408 - val_acc: 0.8268\n",
      "Epoch 114/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2340 - acc: 0.9107\n",
      "Epoch 00114: val_acc did not improve\n",
      "29/29 [==============================] - 14s 474ms/step - loss: 0.2332 - acc: 0.9116 - val_loss: 0.4382 - val_acc: 0.8268\n",
      "Epoch 115/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2297 - acc: 0.9096\n",
      "Epoch 00115: val_acc did not improve\n",
      "29/29 [==============================] - 13s 464ms/step - loss: 0.2305 - acc: 0.9095 - val_loss: 0.4405 - val_acc: 0.8242\n",
      "Epoch 116/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2289 - acc: 0.9139\n",
      "Epoch 00116: val_acc did not improve\n",
      "29/29 [==============================] - 13s 451ms/step - loss: 0.2273 - acc: 0.9136 - val_loss: 0.4404 - val_acc: 0.8242\n",
      "Epoch 117/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2446 - acc: 0.9043\n",
      "Epoch 00117: val_acc did not improve\n",
      "29/29 [==============================] - 13s 445ms/step - loss: 0.2435 - acc: 0.9046 - val_loss: 0.4411 - val_acc: 0.8255\n",
      "Epoch 118/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2353 - acc: 0.9051\n",
      "Epoch 00118: val_acc did not improve\n",
      "29/29 [==============================] - 13s 455ms/step - loss: 0.2425 - acc: 0.9009 - val_loss: 0.4423 - val_acc: 0.8268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2274 - acc: 0.9199\n",
      "Epoch 00119: val_acc did not improve\n",
      "29/29 [==============================] - 13s 453ms/step - loss: 0.2276 - acc: 0.9205 - val_loss: 0.4429 - val_acc: 0.8203\n",
      "Epoch 120/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2329 - acc: 0.9139\n",
      "Epoch 00120: val_acc did not improve\n",
      "29/29 [==============================] - 13s 450ms/step - loss: 0.2360 - acc: 0.9115 - val_loss: 0.4427 - val_acc: 0.8255\n",
      "Epoch 121/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2335 - acc: 0.9072\n",
      "Epoch 00121: val_acc did not improve\n",
      "29/29 [==============================] - 13s 446ms/step - loss: 0.2279 - acc: 0.9104 - val_loss: 0.4400 - val_acc: 0.8268\n",
      "Epoch 122/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2397 - acc: 0.9040\n",
      "Epoch 00122: val_acc did not improve\n",
      "29/29 [==============================] - 17s 593ms/step - loss: 0.2394 - acc: 0.9052 - val_loss: 0.4400 - val_acc: 0.8242\n",
      "Epoch 123/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2534 - acc: 0.8976\n",
      "Epoch 00123: val_acc did not improve\n",
      "29/29 [==============================] - 17s 588ms/step - loss: 0.2507 - acc: 0.8989 - val_loss: 0.4407 - val_acc: 0.8281\n",
      "Epoch 124/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2363 - acc: 0.9043\n",
      "Epoch 00124: val_acc did not improve\n",
      "29/29 [==============================] - 17s 577ms/step - loss: 0.2364 - acc: 0.9055 - val_loss: 0.4424 - val_acc: 0.8229\n",
      "Epoch 125/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2381 - acc: 0.9047\n",
      "Epoch 00125: val_acc did not improve\n",
      "29/29 [==============================] - 18s 614ms/step - loss: 0.2380 - acc: 0.9047 - val_loss: 0.4410 - val_acc: 0.8242\n",
      "Epoch 126/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2404 - acc: 0.9083\n",
      "Epoch 00126: val_acc did not improve\n",
      "29/29 [==============================] - 17s 582ms/step - loss: 0.2458 - acc: 0.9061 - val_loss: 0.4394 - val_acc: 0.8320\n",
      "Epoch 127/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2170 - acc: 0.9129\n",
      "Epoch 00127: val_acc did not improve\n",
      "29/29 [==============================] - 16s 561ms/step - loss: 0.2167 - acc: 0.9127 - val_loss: 0.4430 - val_acc: 0.8242\n",
      "Epoch 128/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2604 - acc: 0.9069\n",
      "Epoch 00128: val_acc did not improve\n",
      "29/29 [==============================] - 17s 578ms/step - loss: 0.2609 - acc: 0.9058 - val_loss: 0.4424 - val_acc: 0.8281\n",
      "Epoch 129/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2652 - acc: 0.8951\n",
      "Epoch 00129: val_acc did not improve\n",
      "29/29 [==============================] - 16s 565ms/step - loss: 0.2627 - acc: 0.8976 - val_loss: 0.4400 - val_acc: 0.8307\n",
      "Epoch 130/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2330 - acc: 0.9188\n",
      "Epoch 00130: val_acc improved from 0.83203 to 0.83333, saving model to Machine Learning InceptionV3\\models\\model_ex-130_acc-0.833333.h5\n",
      "29/29 [==============================] - 16s 562ms/step - loss: 0.2327 - acc: 0.9194 - val_loss: 0.4412 - val_acc: 0.8333\n",
      "Epoch 131/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2316 - acc: 0.9242\n",
      "Epoch 00131: val_acc did not improve\n",
      "29/29 [==============================] - 16s 538ms/step - loss: 0.2286 - acc: 0.9246 - val_loss: 0.4387 - val_acc: 0.8255\n",
      "Epoch 132/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2410 - acc: 0.9054\n",
      "Epoch 00132: val_acc did not improve\n",
      "29/29 [==============================] - 16s 537ms/step - loss: 0.2444 - acc: 0.9043 - val_loss: 0.4378 - val_acc: 0.8268\n",
      "Epoch 133/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2480 - acc: 0.9029\n",
      "Epoch 00133: val_acc did not improve\n",
      "29/29 [==============================] - 15s 531ms/step - loss: 0.2466 - acc: 0.9041 - val_loss: 0.4397 - val_acc: 0.8255\n",
      "Epoch 134/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2524 - acc: 0.9043\n",
      "Epoch 00134: val_acc did not improve\n",
      "29/29 [==============================] - 15s 514ms/step - loss: 0.2483 - acc: 0.9065 - val_loss: 0.4415 - val_acc: 0.8307\n",
      "Epoch 135/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2194 - acc: 0.9152\n",
      "Epoch 00135: val_acc did not improve\n",
      "29/29 [==============================] - 15s 523ms/step - loss: 0.2187 - acc: 0.9149 - val_loss: 0.4394 - val_acc: 0.8281\n",
      "Epoch 136/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2792 - acc: 0.8933\n",
      "Epoch 00136: val_acc did not improve\n",
      "29/29 [==============================] - 15s 509ms/step - loss: 0.2784 - acc: 0.8938 - val_loss: 0.4399 - val_acc: 0.8242\n",
      "Epoch 137/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2236 - acc: 0.9188\n",
      "Epoch 00137: val_acc did not improve\n",
      "29/29 [==============================] - 15s 507ms/step - loss: 0.2227 - acc: 0.9205 - val_loss: 0.4388 - val_acc: 0.8255\n",
      "Epoch 138/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2209 - acc: 0.9224\n",
      "Epoch 00138: val_acc did not improve\n",
      "29/29 [==============================] - 14s 493ms/step - loss: 0.2221 - acc: 0.9197 - val_loss: 0.4383 - val_acc: 0.8294\n",
      "Epoch 139/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2294 - acc: 0.9051\n",
      "Epoch 00139: val_acc did not improve\n",
      "29/29 [==============================] - 14s 494ms/step - loss: 0.2292 - acc: 0.9069 - val_loss: 0.4380 - val_acc: 0.8307\n",
      "Epoch 140/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2356 - acc: 0.9163\n",
      "Epoch 00140: val_acc did not improve\n",
      "29/29 [==============================] - 14s 490ms/step - loss: 0.2386 - acc: 0.9149 - val_loss: 0.4368 - val_acc: 0.8281\n",
      "Epoch 141/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2319 - acc: 0.9043\n",
      "Epoch 00141: val_acc did not improve\n",
      "29/29 [==============================] - 14s 486ms/step - loss: 0.2289 - acc: 0.9054 - val_loss: 0.4361 - val_acc: 0.8255\n",
      "Epoch 142/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2409 - acc: 0.9123- ETA: 6s - l\n",
      "Epoch 00142: val_acc did not improve\n",
      "29/29 [==============================] - 14s 478ms/step - loss: 0.2439 - acc: 0.9111 - val_loss: 0.4422 - val_acc: 0.8255\n",
      "Epoch 143/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2186 - acc: 0.9177\n",
      "Epoch 00143: val_acc did not improve\n",
      "29/29 [==============================] - 14s 480ms/step - loss: 0.2158 - acc: 0.9194 - val_loss: 0.4385 - val_acc: 0.8281\n",
      "Epoch 144/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2643 - acc: 0.8938\n",
      "Epoch 00144: val_acc did not improve\n",
      "29/29 [==============================] - 14s 471ms/step - loss: 0.2612 - acc: 0.8964 - val_loss: 0.4394 - val_acc: 0.8294\n",
      "Epoch 145/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2229 - acc: 0.9161\n",
      "Epoch 00145: val_acc did not improve\n",
      "29/29 [==============================] - 13s 459ms/step - loss: 0.2235 - acc: 0.9169 - val_loss: 0.4380 - val_acc: 0.8307\n",
      "Epoch 146/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2423 - acc: 0.9065\n",
      "Epoch 00146: val_acc did not improve\n",
      "29/29 [==============================] - 13s 454ms/step - loss: 0.2434 - acc: 0.9054 - val_loss: 0.4355 - val_acc: 0.8320\n",
      "Epoch 147/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2207 - acc: 0.9159\n",
      "Epoch 00147: val_acc did not improve\n",
      "29/29 [==============================] - 13s 459ms/step - loss: 0.2216 - acc: 0.9155 - val_loss: 0.4361 - val_acc: 0.8294\n",
      "Epoch 148/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2349 - acc: 0.9087\n",
      "Epoch 00148: val_acc did not improve\n",
      "29/29 [==============================] - 13s 464ms/step - loss: 0.2385 - acc: 0.9065 - val_loss: 0.4380 - val_acc: 0.8307\n",
      "Epoch 149/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2304 - acc: 0.9092\n",
      "Epoch 00149: val_acc did not improve\n",
      "29/29 [==============================] - 13s 459ms/step - loss: 0.2301 - acc: 0.9080 - val_loss: 0.4384 - val_acc: 0.8281\n",
      "Epoch 150/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2412 - acc: 0.9076\n",
      "Epoch 00150: val_acc did not improve\n",
      "29/29 [==============================] - 13s 456ms/step - loss: 0.2363 - acc: 0.9108 - val_loss: 0.4377 - val_acc: 0.8320\n",
      "Epoch 151/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2357 - acc: 0.9038\n",
      "Epoch 00151: val_acc did not improve\n",
      "29/29 [==============================] - 13s 444ms/step - loss: 0.2375 - acc: 0.9028 - val_loss: 0.4396 - val_acc: 0.8320\n",
      "Epoch 152/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2427 - acc: 0.9074\n",
      "Epoch 00152: val_acc did not improve\n",
      "29/29 [==============================] - 17s 598ms/step - loss: 0.2459 - acc: 0.9063 - val_loss: 0.4373 - val_acc: 0.8333\n",
      "Epoch 153/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2271 - acc: 0.9076\n",
      "Epoch 00153: val_acc did not improve\n",
      "29/29 [==============================] - 17s 589ms/step - loss: 0.2256 - acc: 0.9086 - val_loss: 0.4398 - val_acc: 0.8307\n",
      "Epoch 154/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2659 - acc: 0.8880\n",
      "Epoch 00154: val_acc improved from 0.83333 to 0.83464, saving model to Machine Learning InceptionV3\\models\\model_ex-154_acc-0.834635.h5\n",
      "29/29 [==============================] - 17s 597ms/step - loss: 0.2634 - acc: 0.8908 - val_loss: 0.4358 - val_acc: 0.8346\n",
      "Epoch 155/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2475 - acc: 0.8909\n",
      "Epoch 00155: val_acc did not improve\n",
      "29/29 [==============================] - 17s 576ms/step - loss: 0.2515 - acc: 0.8871 - val_loss: 0.4349 - val_acc: 0.8320\n",
      "Epoch 156/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2453 - acc: 0.9105\n",
      "Epoch 00156: val_acc did not improve\n",
      "29/29 [==============================] - 17s 573ms/step - loss: 0.2479 - acc: 0.9093 - val_loss: 0.4389 - val_acc: 0.8281\n",
      "Epoch 157/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2291 - acc: 0.9174\n",
      "Epoch 00157: val_acc did not improve\n",
      "29/29 [==============================] - 17s 587ms/step - loss: 0.2353 - acc: 0.9138 - val_loss: 0.4395 - val_acc: 0.8333\n",
      "Epoch 158/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2522 - acc: 0.8929\n",
      "Epoch 00158: val_acc did not improve\n",
      "29/29 [==============================] - 18s 615ms/step - loss: 0.2588 - acc: 0.8890 - val_loss: 0.4388 - val_acc: 0.8268\n",
      "Epoch 159/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2348 - acc: 0.9034\n",
      "Epoch 00159: val_acc did not improve\n",
      "29/29 [==============================] - 16s 553ms/step - loss: 0.2352 - acc: 0.9035 - val_loss: 0.4387 - val_acc: 0.8281\n",
      "Epoch 160/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2204 - acc: 0.9110\n",
      "Epoch 00160: val_acc did not improve\n",
      "29/29 [==============================] - 16s 563ms/step - loss: 0.2210 - acc: 0.9097 - val_loss: 0.4383 - val_acc: 0.8320\n",
      "Epoch 161/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2578 - acc: 0.8984\n",
      "Epoch 00161: val_acc did not improve\n",
      "29/29 [==============================] - 17s 571ms/step - loss: 0.2594 - acc: 0.8945 - val_loss: 0.4371 - val_acc: 0.8281\n",
      "Epoch 162/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2415 - acc: 0.9049\n",
      "Epoch 00162: val_acc did not improve\n",
      "29/29 [==============================] - 16s 542ms/step - loss: 0.2394 - acc: 0.9061 - val_loss: 0.4388 - val_acc: 0.8255\n",
      "Epoch 163/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2516 - acc: 0.9074\n",
      "Epoch 00163: val_acc did not improve\n",
      "29/29 [==============================] - 16s 538ms/step - loss: 0.2482 - acc: 0.9095 - val_loss: 0.4360 - val_acc: 0.8333\n",
      "Epoch 164/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2331 - acc: 0.9116\n",
      "Epoch 00164: val_acc did not improve\n",
      "29/29 [==============================] - 15s 531ms/step - loss: 0.2313 - acc: 0.9125 - val_loss: 0.4369 - val_acc: 0.8294\n",
      "Epoch 165/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2389 - acc: 0.9132\n",
      "Epoch 00165: val_acc did not improve\n",
      "29/29 [==============================] - 15s 523ms/step - loss: 0.2383 - acc: 0.9119 - val_loss: 0.4365 - val_acc: 0.8281\n",
      "Epoch 166/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2378 - acc: 0.9088\n",
      "Epoch 00166: val_acc did not improve\n",
      "29/29 [==============================] - 15s 509ms/step - loss: 0.2433 - acc: 0.9055 - val_loss: 0.4373 - val_acc: 0.8320\n",
      "Epoch 167/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2452 - acc: 0.9096\n",
      "Epoch 00167: val_acc did not improve\n",
      "29/29 [==============================] - 15s 502ms/step - loss: 0.2472 - acc: 0.9095 - val_loss: 0.4339 - val_acc: 0.8346\n",
      "Epoch 168/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2348 - acc: 0.9018\n",
      "Epoch 00168: val_acc did not improve\n",
      "29/29 [==============================] - 15s 514ms/step - loss: 0.2370 - acc: 0.8992 - val_loss: 0.4374 - val_acc: 0.8255\n",
      "Epoch 169/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2539 - acc: 0.9125\n",
      "Epoch 00169: val_acc did not improve\n",
      "29/29 [==============================] - 15s 517ms/step - loss: 0.2519 - acc: 0.9134 - val_loss: 0.4357 - val_acc: 0.8333\n",
      "Epoch 170/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2383 - acc: 0.9141\n",
      "Epoch 00170: val_acc did not improve\n",
      "29/29 [==============================] - 14s 492ms/step - loss: 0.2382 - acc: 0.9149 - val_loss: 0.4387 - val_acc: 0.8281\n",
      "Epoch 171/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2321 - acc: 0.9038\n",
      "Epoch 00171: val_acc did not improve\n",
      "29/29 [==============================] - 14s 485ms/step - loss: 0.2328 - acc: 0.9028 - val_loss: 0.4355 - val_acc: 0.8307\n",
      "Epoch 172/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2217 - acc: 0.9174\n",
      "Epoch 00172: val_acc did not improve\n",
      "29/29 [==============================] - 14s 494ms/step - loss: 0.2209 - acc: 0.9181 - val_loss: 0.4396 - val_acc: 0.8242\n",
      "Epoch 173/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2655 - acc: 0.8994\n",
      "Epoch 00173: val_acc did not improve\n",
      "29/29 [==============================] - 14s 477ms/step - loss: 0.2621 - acc: 0.9018 - val_loss: 0.4367 - val_acc: 0.8268\n",
      "Epoch 174/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2431 - acc: 0.8982\n",
      "Epoch 00174: val_acc did not improve\n",
      "29/29 [==============================] - 14s 487ms/step - loss: 0.2466 - acc: 0.8975 - val_loss: 0.4381 - val_acc: 0.8268\n",
      "Epoch 175/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2379 - acc: 0.9031\n",
      "Epoch 00175: val_acc did not improve\n",
      "29/29 [==============================] - 14s 487ms/step - loss: 0.2447 - acc: 0.8989 - val_loss: 0.4372 - val_acc: 0.8281\n",
      "Epoch 176/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2596 - acc: 0.9021\n",
      "Epoch 00176: val_acc did not improve\n",
      "29/29 [==============================] - 14s 494ms/step - loss: 0.2622 - acc: 0.9001 - val_loss: 0.4374 - val_acc: 0.8281\n",
      "Epoch 177/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2459 - acc: 0.8971\n",
      "Epoch 00177: val_acc did not improve\n",
      "29/29 [==============================] - 14s 470ms/step - loss: 0.2482 - acc: 0.8953 - val_loss: 0.4363 - val_acc: 0.8307\n",
      "Epoch 178/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2142 - acc: 0.9299\n",
      "Epoch 00178: val_acc did not improve\n",
      "29/29 [==============================] - 14s 475ms/step - loss: 0.2140 - acc: 0.9302 - val_loss: 0.4367 - val_acc: 0.8307\n",
      "Epoch 179/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2257 - acc: 0.9154\n",
      "Epoch 00179: val_acc did not improve\n",
      "29/29 [==============================] - 15s 533ms/step - loss: 0.2250 - acc: 0.9140 - val_loss: 0.4349 - val_acc: 0.8320\n",
      "Epoch 180/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2214 - acc: 0.9110\n",
      "Epoch 00180: val_acc did not improve\n",
      "29/29 [==============================] - 14s 493ms/step - loss: 0.2267 - acc: 0.9076 - val_loss: 0.4354 - val_acc: 0.8333\n",
      "Epoch 181/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2356 - acc: 0.9141\n",
      "Epoch 00181: val_acc did not improve\n",
      "29/29 [==============================] - 13s 461ms/step - loss: 0.2345 - acc: 0.9127 - val_loss: 0.4363 - val_acc: 0.8294\n",
      "Epoch 182/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2353 - acc: 0.9108\n",
      "Epoch 00182: val_acc did not improve\n",
      "29/29 [==============================] - 17s 600ms/step - loss: 0.2333 - acc: 0.9128 - val_loss: 0.4357 - val_acc: 0.8320\n",
      "Epoch 183/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2489 - acc: 0.9136\n",
      "Epoch 00183: val_acc did not improve\n",
      "29/29 [==============================] - 17s 597ms/step - loss: 0.2461 - acc: 0.9144 - val_loss: 0.4360 - val_acc: 0.8333\n",
      "Epoch 184/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2329 - acc: 0.9121\n",
      "Epoch 00184: val_acc did not improve\n",
      "29/29 [==============================] - 17s 590ms/step - loss: 0.2305 - acc: 0.9140 - val_loss: 0.4351 - val_acc: 0.8307\n",
      "Epoch 185/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2309 - acc: 0.9107\n",
      "Epoch 00185: val_acc improved from 0.83464 to 0.83594, saving model to Machine Learning InceptionV3\\models\\model_ex-185_acc-0.835938.h5\n",
      "29/29 [==============================] - 17s 599ms/step - loss: 0.2287 - acc: 0.9123 - val_loss: 0.4357 - val_acc: 0.8359\n",
      "Epoch 186/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2318 - acc: 0.9090\n",
      "Epoch 00186: val_acc did not improve\n",
      "29/29 [==============================] - 17s 575ms/step - loss: 0.2307 - acc: 0.9100 - val_loss: 0.4352 - val_acc: 0.8346\n",
      "Epoch 187/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2213 - acc: 0.9177\n",
      "Epoch 00187: val_acc did not improve\n",
      "29/29 [==============================] - 17s 571ms/step - loss: 0.2202 - acc: 0.9183 - val_loss: 0.4374 - val_acc: 0.8320\n",
      "Epoch 188/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2451 - acc: 0.9152\n",
      "Epoch 00188: val_acc did not improve\n",
      "29/29 [==============================] - 16s 566ms/step - loss: 0.2454 - acc: 0.9149 - val_loss: 0.4366 - val_acc: 0.8320\n",
      "Epoch 189/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2394 - acc: 0.9023\n",
      "Epoch 00189: val_acc did not improve\n",
      "29/29 [==============================] - 16s 562ms/step - loss: 0.2392 - acc: 0.9013 - val_loss: 0.4393 - val_acc: 0.8268\n",
      "Epoch 190/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2395 - acc: 0.9118\n",
      "Epoch 00190: val_acc did not improve\n",
      "29/29 [==============================] - 16s 551ms/step - loss: 0.2400 - acc: 0.9116 - val_loss: 0.4376 - val_acc: 0.8281\n",
      "Epoch 191/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2250 - acc: 0.9188\n",
      "Epoch 00191: val_acc did not improve\n",
      "29/29 [==============================] - 16s 545ms/step - loss: 0.2277 - acc: 0.9151 - val_loss: 0.4393 - val_acc: 0.8242\n",
      "Epoch 192/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2447 - acc: 0.9031\n",
      "Epoch 00192: val_acc did not improve\n",
      "29/29 [==============================] - 16s 539ms/step - loss: 0.2428 - acc: 0.9054 - val_loss: 0.4407 - val_acc: 0.8242\n",
      "Epoch 193/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2345 - acc: 0.9107\n",
      "Epoch 00193: val_acc did not improve\n",
      "29/29 [==============================] - 16s 539ms/step - loss: 0.2315 - acc: 0.9127 - val_loss: 0.4380 - val_acc: 0.8294\n",
      "Epoch 194/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2725 - acc: 0.8969\n",
      "Epoch 00194: val_acc did not improve\n",
      "29/29 [==============================] - 16s 558ms/step - loss: 0.2688 - acc: 0.8993 - val_loss: 0.4396 - val_acc: 0.8281\n",
      "Epoch 195/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2271 - acc: 0.9217\n",
      "Epoch 00195: val_acc did not improve\n",
      "29/29 [==============================] - 18s 614ms/step - loss: 0.2309 - acc: 0.9190 - val_loss: 0.4346 - val_acc: 0.8333\n",
      "Epoch 196/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2279 - acc: 0.9054\n",
      "Epoch 00196: val_acc did not improve\n",
      "29/29 [==============================] - 16s 559ms/step - loss: 0.2268 - acc: 0.9065 - val_loss: 0.4398 - val_acc: 0.8216\n",
      "Epoch 197/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2205 - acc: 0.9201\n",
      "Epoch 00197: val_acc did not improve\n",
      "29/29 [==============================] - 15s 511ms/step - loss: 0.2215 - acc: 0.9197 - val_loss: 0.4360 - val_acc: 0.8294\n",
      "Epoch 198/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2642 - acc: 0.8875\n",
      "Epoch 00198: val_acc did not improve\n",
      "29/29 [==============================] - 14s 500ms/step - loss: 0.2653 - acc: 0.8871 - val_loss: 0.4415 - val_acc: 0.8307\n",
      "Epoch 199/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2209 - acc: 0.9121\n",
      "Epoch 00199: val_acc did not improve\n",
      "29/29 [==============================] - 14s 494ms/step - loss: 0.2178 - acc: 0.9140 - val_loss: 0.4405 - val_acc: 0.8307\n",
      "Epoch 200/200\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.2572 - acc: 0.9029\n",
      "Epoch 00200: val_acc did not improve\n",
      "29/29 [==============================] - 14s 493ms/step - loss: 0.2617 - acc: 0.9019 - val_loss: 0.4403 - val_acc: 0.8242\n"
     ]
    }
   ],
   "source": [
    "trainer.setDataDirectory(\"Machine Learning InceptionV3\")\n",
    "trainer.trainModel(num_objects=3, num_experiments=200, enhance_data=True, batch_size=32, show_network_summary=True, initial_learning_rate = .000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Variety",
   "language": "python",
   "name": "variety"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
